#!/usr/bin/env python

from os.path import dirname, realpath, exists, abspath, isdir
from os import mkdir, utime, symlink
from shutil import rmtree
from sys import stderr, path

import argparse
import logging
import pysam
import numpy as np
import pandas as pd

from writeDiscordantFragments import writeDiscordantFragments
from formPEClusters import formPEClusters
from consolidatePEClusters import consolidatePEClusters
from uniqueSuppFilter import uniqueSuppFilter
from writeBEDs import writeBEDs
from addSplitReads import addSplitReads
from covPUFilter import covPUFilter

def createDirectory(name):
    try:
        mkdir(name)
    except OSError:
        return None
    return name

def createDiscordants():
    logging.info('Started writing the BAM files')

    # what is the sort order of this file?
    samfile = pysam.AlignmentFile(ARGS.disc, "rb")
    try:
        sortorder = samfile.header['HD']['SO']
    except KeyError:
        print >> stderr, "Missing headers. Please check if BAM was correctly written."
        exit(1)
    samfile.close()

    discfile = ARGS.disc
    print "Sort order:", sortorder
    if sortorder != 'queryname':
        discfile = "%s/discordants.ns.bam" % WORKSPACE
        logging.info('Started name sorting the discordant file')
        pysam.sort("-n", "-O", "bam", "-T", WORKSPACE + "/xxx", \
                    "-@", str(ARGS.threads), "-o", discfile, ARGS.disc)
        logging.info('Finished name sorting the discordant file')

    # remove almts without left or right partner
    newDiscFile = "%s/discordants.dedup.ns.bam" % WORKSPACE
    samfile1 = pysam.AlignmentFile(discfile , 'rb')
    samfile2 = pysam.AlignmentFile(newDiscFile, 'wb', template = samfile1)
    almtList = []
    prev_query_name = None
    read1_occurred, read2_occurred = 0,0
    for almt in samfile1:
        if almt.query_name == prev_query_name or prev_query_name is None:
            almtList.append(almt)
        elif almt.query_name != prev_query_name and \
            read1_occurred == True and read2_occurred == True:
            for entry in almtList:
                samfile2.write(entry)
            almtList = [almt]
            read1_occurred, read2_occurred = 0,0
        elif almt.query_name != prev_query_name:
            almtList = [almt]
            read1_occurred, read2_occurred = 0,0
        prev_query_name = almt.query_name
        if almt.is_read1:
            read1_occurred = 1
        elif almt.is_read2:
            read2_occurred = 1
    samfile1.close()
    samfile2.close()

    discfile = newDiscFile
    samfile = pysam.AlignmentFile(discfile, 'rb')
    outfile1 = pysam.AlignmentFile("%s/aln1s.bam" % WORKSPACE, 'wb', template=samfile)
    outfile2 = pysam.AlignmentFile("%s/aln2s.bam" % WORKSPACE, 'wb', template=samfile)
    logging.info('Started writing the discordant reads')

    for alignment in samfile:
        if alignment.is_read1:
            outfile1.write(alignment)
        else:
            outfile2.write(alignment)
    samfile.close()
    outfile1.close()
    outfile2.close()
    logging.info('Finished writing the discordant reads')

    logging.info('Finished writing the BAM files')

def printVCFHeader(f):
    print >> f, "##fileformat=VCF4.3"
    print >> f, "##source=SVXplorer-" + VERSION
    print >> f, """##INFO=<ID=END, Number=1, Type=Integer, Description=\"end point of SV\">
##INFO=<ID=SVTYPE, Number=1, Type=String, Description=\"SV Type\">
##INFO=<ID=CM, Number=1, Type=String, Description=\"SV Type for all entries with current GROUPID combined, e.g. 'Cut-paste insertion'\">
##INFO=<ID=PROBTYPE, Number=1, Type=String, Description=\"Likely or possible SV Type for BND event\"> 
##INFO=<ID=ISINV, Number=1, Type=Flag, Description=\"Whether on inverted or positive strand\">
##INFO=<ID=CHR2, Number=1, Type=Integer, Description=\"For BNDs the reference ID of the 'END' breakpoint if different from that of start 'POS'\">
##INFO=<ID=GROUPID, Number=1, Type=String, Description=\"GROUPID correlating 2 translocation or non-tandem-duplication events\">
##INFO=<ID=SVLEN,Number=.,Type=Integer,Description="Difference in length between REF and ALT alleles">
##INFO=<ID=IMPRECISE,Number=0,Type=Flag,Description="Imprecise structural variation">
##INFO=<ID=PRECISE,Number=0,Type=Flag,Description="Precise structural variation">
##INFO=<ID=CIPOS,Number=2,Type=Integer,Description="Confidence interval around POS for imprecise variants">
##INFO=<ID=CIEND,Number=2,Type=Integer,Description="Confidence interval around END for imprecise variants">
##INFO=<ID=MATEID,Number=.,Type=String,Description="ID of mate breakends for BND events">
##INFO=<ID=SUPPORT,Number=.,Type=Integer,Description="PE+SR">
##INFO=<ID=PE,Number=.,Type=Integer,Description="Number of paired-end reads supporting the variant">
##INFO=<ID=SR,Number=.,Type=Integer,Description="Number of split reads supporting the variant">
##ALT=<ID=<DEL>,Description="Deletion">
##ALT=<ID=<DUP>,Description="Duplication">
##ALT=<ID=<INV>,Description="Inversion">
##ALT=<ID=<DUP:TANDEM>,Description="Tandem duplication">
##ALT=<ID=INS,Description="Insertion of novel sequence">
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
"""
    print >> f, "\t".join(["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO"])

def writeVCFFromBedpe(inputFile, outputFile):
    """Read the BEDPE and convert to VCF."""
    with open(inputFile, 'r') as inpt, open(outputFile,'w') as otpt:
        counter = 0
        printVCFHeader(otpt)
        for line in inpt:
            counter+=1
            tokens = line.split()
            precise=tokens[11].find("SR")
            support="SUPPORT=" + tokens[16] + ";PE=" + tokens[19] + ";SR=" + tokens[20] + ";"  
            chr1 = tokens[0]
            chr1Start = tokens[1]
            chr1End = tokens[2]
            chr2Start = tokens[4]
            chr2End = tokens[5]
            name = tokens[10]
            bnd = tokens[17]
            CM = tokens[18]
            cl_support = tokens[21]
            cipos = str(int(chr1End)-int(chr1Start))
            svlen = str(abs(int(chr2End) - int(chr1Start)))

            if precise == -1:
                precise = "IMPRECISE"
            else:
                precise="PRECISE"

            chr2=""
            if chr1 != chr2:
                chr2="CHR2="+ tokens[3] + ";"

            if name == "BND":
                GROUPID = ""
                if CM.startswith("INS_C"):
                    CM = "Translocation"
                    if tokens[24][0] == "T":
                        GROUPID = "GROUPID=" + tokens[24] + ";"
                elif CM.startswith("INS") or CM.startswith("TD"):
                    CM = "Duplication"
                elif CM.startswith("INV"):
                    CM = "Inversion"
                if tokens[22] != "." and tokens[23] != ".":
                    BNDAlt1, BNDAlt2 = tokens[22].replace("p", tokens[3] + ":" + chr2End),\
                    tokens[23].replace("p", chr1 + ":" + chr1Start)
                else:
                    BNDAlt1, BNDAlt2 = ".", "."

                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1Start, counter, "N", BNDAlt1, ".","PASS", "SVTYPE=BND;CIPOS=0," + cipos + ";CIEND=-" + cipos + ",0;PROBTYPE=" + CM + ";MATEID=" + str(counter + 1) + ";" + GROUPID + support + precise)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (tokens[3], chr2End, counter + 1, "N", BNDAlt2, ".","PASS", "SVTYPE=BND;CIPOS=0," + cipos + ";CIEND=-" + cipos + ",0;PROBTYPE=" + CM + ";MATEID=" + str(counter) + ";" + GROUPID + support + precise)
                counter+= 1
            elif name == "DEL":
                 print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1Start,counter,"N", "<DEL>",".","PASS", "SVTYPE=DEL;END=" + chr2End + ";SVLEN=-" + svlen + ";CIPOS=0," + cipos + ";CIEND=-" + cipos + ",0;" + support + precise)
            elif name == "TD" or name == "TD_INV":
                isinv=""
                if name=="TD_INV":
                    isinv="ISINV"
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1End,counter,"N", "<DUP:TANDEM>",".","PASS", "SVTYPE=DUP;END=" + chr2Start + ";SVLEN=" + svlen + ";CIPOS=-" + cipos + ",0;CIEND=0," + cipos + ";" + support + precise)
            elif name == "INV":
                ciend = int(chr2End) - int(chr2Start)
                pos = int((int(chr1Start) + int(chr1End))/2.0)
                end = int((int(chr2Start) + int(chr2End))/2.0)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, pos, counter,"N", "<INV>",".","PASS", "SVTYPE=INV;END=" + str(end) + ";CIPOS=-" + str(int(cipos)/2.0) +"," + str(int(cipos)/2.0) + ";CIEND=-" + str(int(ciend)/2.0) +"," + str(int(ciend)/2.0) + ";" + support + precise)
            elif name in ["INS","INS_I","INS_C_P","INS_C_I_P"]:
                if name in ["INS","INS_I"]:
                    field1 = "DUP"
                    CM = "CopyPasteInsertion"
                    # if paste < source in case on same chr
                else:
                    field1 = "DEL"
                    CM = "CutPasteInsertion"
                svlen=str(int(chr1End)-int(chr1Start))
                cipos = int(chr2End)-int(chr2Start)
                isinv=""
                if name=="INS_I":
                    isinv="ISINV"
                 
                BNDAlt1, BNDAlt2 = "N[" + chr1 + ":" + chr1Start + "[", "]" + tokens[3] + ":" + chr2Start + "]N"
                BNDAlt3, BNDAlt4 = "]" + tokens[3] + ":" + chr2Start + "]N", "N[" + chr1 + ":" + chr1End + "["
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1Start,counter,"N", "<" + field1 + ">", ".","PASS", "SVTYPE=" + field1 + ";END=" + chr1End + ";SVLEN=" + svlen + ";CIPOS=0," + str(cipos) + ";CIEND=-" + str(cipos) +",0;GROUPID=" + str(counter) + ";" + isinv + ";" + support + precise)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (tokens[3], chr2Start, counter + 1,"N", BNDAlt1,".","PASS", "SVTYPE=BND;CM=" + CM + ";SVLEN=" + svlen + ";CIPOS=0," + str(cipos) + ";CIEND=0," + str(cipos) + ";GROUPID=" + str(counter) + ";MATEID=" + str(counter + 2) + ";" + isinv + ";" + support + precise)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1Start, counter + 2,"N", BNDAlt2, ".","PASS", "SVTYPE=BND;CM=" + CM + ";SVLEN=" + svlen + ";CIPOS=0," + str(cipos) + ";CIEND=0," + str(cipos) + ";GROUPID=" + str(counter) + ";MATEID=" + str(counter + 1) + isinv + ";" + support + precise)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (chr1, chr1End, counter + 3,"N", BNDAlt3, ".","PASS", "SVTYPE=BND;CM=" + CM + ";SVLEN=" + svlen + ";CIPOS=0," + str(cipos) + ";CIEND=0," + str(cipos) + ";GROUPID=" + str(counter) + ";MATEID=" + str(counter + 4) + ";" + isinv + ";" + support + precise)
                print >> otpt, "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s" % (tokens[3], chr2Start, counter + 4,"N", BNDAlt4, ".","PASS", "SVTYPE=BND;CM=" + CM + ";SVLEN=" + svlen + ";CIPOS=0," + str(cipos) + ";CIEND=0," + str(cipos) + ";GROUPID=" + str(counter) + ";MATEID=" + str(counter + 3) + isinv + ";" + support + precise)
                counter+= 4
            else:
                print>>stderr, "Unrecognized SV type"
                exit(1)

def filterAndFormat(variantMapFile, allVariantFile, statFile, midfix):
    if variantMapFile is not None:
        # pick variants that have the minimum unique support. This writes
        # 1. variants.uniqueFilter.txt
        # 2. variants.uniqueSupport.txt
        uniqueSuppFilter(WORKSPACE, statFile, variantMapFile, allVariantFile,
                         "%s/allDiscordants.txt" % WORKSPACE, MAP_THRESH_U, 
                          MIN_UNIQUE_SUPP, 6, 6, 3, 3, 4, 4, 3000000, 100000000, -1)

    # write the results. This writes
    # 1. variants.bedpe
    passedFile = "%s/variants.uniqueFilter.txt" % WORKSPACE
    bedpeFile = "%s/variants.%s.bedpe" % (WORKSPACE, midfix)
    writeBEDs(allVariantFile, passedFile, bedpeFile, LIB_INV)

    # write a VCF file
    # 1. variants.vcf
    vcfFile = "%s/variants.%s.vcf" % (WORKSPACE, midfix)
    writeVCFFromBedpe(bedpeFile, vcfFile)

def processFragments():
    # create discordant fragments
    createDiscordants()

    # write the discordant fragments in a simple format. This should create:
    # 1. allDiscordants.us.txt : fragments that are discordant (unsorted)
    # 2. allDiscordants.up.us.txt : fragments where only one end maps (unsorted).
    readAlmts1 = "%s/aln1s.bam" % WORKSPACE
    readAlmts2 = "%s/aln2s.bam" % WORKSPACE
    writeDiscordantFragments(WORKSPACE, readAlmts1, readAlmts2, ARGS.samplebam,
                             ARGS.d, ARGS.i, ARGS.c, PE_ALMT_COMB_THRESH, 
                             CALC_THRESH, NMATCH_PCT_THRESH,
                             NMATCH_RELATIVE_THRESH, AS_RELATIVE_THRESH, 
                             MAP_THRESH)

    # sort the allDiscordants.us.txt file -> allDiscordants.txt
    logging.info('Started sorting the discordants')
    data = pd.read_table("%s/allDiscordants.us.txt" % WORKSPACE, 
                         names=['index', 'lchr', 'lpos', 'rchr', 'rpos', 
                                'orient', 'small', 'mapq'],
                         dtype={'index':np.int32, 'lchr':np.str, 'lpos':np.int32,
                                'rchr':np.str, 'rpos':np.int32, 'orient':np.str,
                                'mapq': np.int16})
    data = data.sort_values(by = ['lchr', 'rchr', 'lpos', 'rpos'])
    data.to_csv("%s/allDiscordants.txt" % WORKSPACE, header=None, index=None, sep='\t')

    # sort the allDiscordants.up.us.txt file -> allDiscordants.up.txt
    data = pd.read_table("%s/allDiscordants.up.us.txt" % WORKSPACE,
                         names=['index', 'lchr', 'lpos', 'rchr', 'rpos', 
                                'orient', 'small', 'mapq'],
                         dtype={'index':np.int32, 'lchr':np.str, 'lpos':np.int32,
                                'rchr':np.str, 'rpos':np.str, 'orient':np.str,
                                'mapq': np.int16})
    data = data.sort_values(by = ['lchr', 'lpos'])
    data.to_csv("%s/allDiscordants.up.txt" % WORKSPACE, header=None, index=None, sep='\t')
    logging.info('Finished sorting the discordants')

    # form PE clusters from those discordant fragments. Creates
    # 1. allClusters.txt
    # 2. clusterMap.txt
    # 3. clusterCliques.txt in debug mode 
    statFile = "%s/bamStats.txt" % WORKSPACE 
    binFile = "%s/binDist.txt" % WORKSPACE
    formPEClusters(WORKSPACE, statFile, binFile, MIN_CS, DISC_ENHANCER, MIN_PE_BPMARGIN, ARGS.subsample, ARGS.d)

    # collect the clusters that pass requirements -> allClusters.thresh.txt
    data = pd.read_table("%s/allClusters.txt" % WORKSPACE, 
                         names=['index', 'ns', 'orient', 'lchr', 'lpos', 'lend',
                                'rchr', 'rpos', 'rend', 'small'],
                         dtype={'lchr':np.str, 'rchr':np.str, 'orient':np.str})
    data = data[data['ns'] >= MIN_CS]
    data.to_csv("%s/allClusters.thresh.txt" % WORKSPACE, header=None, index=None, sep='\t')

    # calculate the distance you should walk back to compare variants to each 
    # other
    df = data['lend'] - data['lpos']
    max_cl_comb_gap = df.max()
    logging.info('Setting max_cl_comb_gap to %f', max_cl_comb_gap)


    # sort the clusters based on the left breakpoint -> allClusters.ls.txt
    clusterFileLS = "%s/allClusters.ls.txt" % WORKSPACE
    data = pd.read_table("%s/allClusters.thresh.txt" % WORKSPACE,
                         names=['index', 'ns', 'orient', 'lchr', 'lpos', 'lend',
                                'rchr', 'rpos', 'rend', 'small'],
                         dtype={'lchr':np.str, 'rchr':np.str, 'orient':np.str})
    data = data.sort_values(by = ['lchr', 'lpos'])
    data.to_csv(clusterFileLS, header=None, index=None, sep='\t')

    # sort the clusters based on the right breakpoint -> allClusters.rs.txt
    clusterFileRS = "%s/allClusters.rs.txt" % WORKSPACE
    data = pd.read_table("%s/allClusters.thresh.txt" % WORKSPACE,
                         names=['index', 'ns', 'orient', 'lchr', 'lpos', 'lend',
                                'rchr', 'rpos', 'rend', 'small'],
                         dtype={'lchr':np.str, 'rchr':np.str, 'orient':np.str})
    data = data.sort_values(by = ['rchr', 'rpos'])
    data.to_csv(clusterFileRS, header=None, index=None, sep='\t')

    # consolidate those clusters in to variants. Creates
    # 1. allVariants.pe.txt
    # 2. variantMap.pe.txt
    # 3. claimedClusters.txt
    clusterMapFile = "%s/clusterMap.txt" % WORKSPACE
    consolidatePEClusters(WORKSPACE, statFile, clusterFileLS, clusterFileRS,
                          clusterMapFile, max_cl_comb_gap, SLOP_PE, REF_RATE_PE, AS_RELATIVE_THRESH)

    # filter and format the results
    variantMapFile = "%s/variantMap.pe.txt" % WORKSPACE
    allVariantFile = "%s/allVariants.pe.txt" % WORKSPACE
    filterAndFormat(variantMapFile, allVariantFile, statFile, "pe")

    # name sort the BAM file if it is not name-sorted. 
    samfile = pysam.AlignmentFile(ARGS.split, 'rb')
    try:
        sortorder = samfile.header['HD']['SO']
    except KeyError:
        print >> stderr, "Missing headers. Please check if BAM was correctly written."
        exit(1)
    samfile.close()

    splitfile = ARGS.split
    if sortorder == 'coordinate':
        splitfile = "%s/splitters.ns.bam" % WORKSPACE
        logging.info('Started name sorting the splitters file')
        pysam.sort("-n", "-O", "bam", "-T", "xxx", "-o", splitfile, ARGS.split)
        logging.info('Finished name sorting the splitters file')

    # now add the split read information to the system. Write the files 
    # 1. variantMap.pe_sr.txt
    # 2. allVariants.pe_sr.txt
    addSplitReads(WORKSPACE, variantMapFile, allVariantFile, splitfile,
                  SLOP_SR, REF_RATE_SR, MIN_VS_SR, MQ_SR, ARGS.c,
                  MIN_SIZE_INS_SR, MIN_SRtoPE_SUPP, ARGS.i)

    # filter and format these results
    filterAndFormat("%s/variantMap.pe_sr.txt" % WORKSPACE, 
                    "%s/allVariants.pe_sr.txt" % WORKSPACE, statFile, "pe_sr")

    variantMapFile = "%s/variantMap.pe_sr.txt" % WORKSPACE
    allVariantFile = "%s/allVariants.pe_sr.txt" % WORKSPACE
    uniqueVariantFile = "%s/variants.uniqueFilter.txt" % WORKSPACE
    covPUFilter(WORKSPACE, allVariantFile, variantMapFile, uniqueVariantFile,
                statFile, ARGS.samplebam, ARGS.m, DEL_CN_SUPP_THRESH,
                DUP_CN_SUPP_THRESH, SPLIT_INS, PILEUP_THRESH, GOOD_REG_THRESH, ARGS.minVarSize)

    # filter and format these results
    filterAndFormat(None, "%s/allVariants.pu.txt" % WORKSPACE, statFile, "pu")

if __name__ == '__main__':
    # set the name of the directory where this script lives
    SCRIPT_DIR = dirname(realpath(__file__))

    # set the VERSION
    with open(path[0]+'/VERSION',"r") as version_file:
        VERSION = version_file.read().strip()    

    # $$$ add option to print version and exit
    PARSER = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description='Identify SVs in a sample using paired-end reads, split reads and local read depth')

    PARSER.add_argument('-d', action='store_true',
                        help='print debug information')
    PARSER.add_argument('-f', action='store_true',
                        help='overwrite existing workspace')

    PARSER.add_argument('-i', default=None, 
                        help='ignore regions in this BED file')
    PARSER.add_argument('-c', default=None,
                        help='ignore the chromosomes')
    PARSER.add_argument('-m', default=None,
                        help='mappable intervals in a BED file')
    PARSER.add_argument('-w', default="svxplorer", help='use dir as workspace')
    PARSER.add_argument('-s', default=100, dest='minVarSize', type=int, help='minimum size of variants called')
    PARSER.add_argument('-t', default=1, dest='threads', type=int, help='number of threads for samtools')
    PARSER.add_argument('disc', help='bam file of discordant pairs')
    PARSER.add_argument('split', help='bam file of split reads')
    PARSER.add_argument('samplebam', help='bam file of alignments')
    PARSER.add_argument('reference', help='path to reference genome')

    # options we do not expect the user to change
    PARSER.add_argument('--subsample', action='store_true', help=argparse.SUPPRESS)    

    # writeDiscordantFragments
    CALC_THRESH=1000000
    MAP_THRESH=1
    # useful for secondary alignments
    PE_ALMT_COMB_THRESH=10 #20
    NMATCH_RELATIVE_THRESH=0
    NMATCH_PCT_THRESH=0
    AS_RELATIVE_THRESH=2
    
    # formPEClusters
    MIN_CS=3
    DISC_ENHANCER=1.67
    MIN_PE_BPMARGIN=25

    # consolidatePEClusters
    SLOP_PE=0
    REF_RATE_PE=5

    # uniqueSuppFilter (PE)
    MAP_THRESH_U=-1
    MIN_UNIQUE_SUPP=3

    # writeBEDs
    LIB_INV=True

    # addSplitReads
    SLOP_SR=8
    REF_RATE_SR=0
    MIN_VS_SR=3
    MQ_SR=10
    MIN_SIZE_INS_SR=30
    MIN_SRtoPE_SUPP=2

    # covPUFilter
    DEL_CN_SUPP_THRESH=.6 
    DUP_CN_SUPP_THRESH=1.4
    SPLIT_INS=0
    PILEUP_THRESH=500.0
    GOOD_REG_THRESH=.8

    ARGS = PARSER.parse_args()

    # start logging
    if ARGS.d:
        LEVEL = logging.DEBUG
    else:
        LEVEL = logging.INFO
    LOGMODE = 'w'

    # check if BAM and reference match
    CHROMS = pd.read_table("%s.fai" % ARGS.reference,
                           usecols=[0], names=['chrom'])
    CHROMS = CHROMS['chrom'].tolist()
    bamfile = pysam.AlignmentFile(ARGS.samplebam, "rb")
    bamsn = [x['SN'] for x in bamfile.header['SQ']]
    correctbam = True
    for chrom in bamsn:
        if chrom not in CHROMS:
            correctbam = False
            break
    if correctbam == False: 
        print >> stderr, "Error: All chromosomes were not found in the specified reference"
        exit(1)
    bamfile.close()

    # create the workspace
    createDirResponse = createDirectory(ARGS.w)
    if createDirResponse == None and isdir(ARGS.w):
        if ARGS.f:
            print >> stderr, "Overwriting existing output directory"
            rmtree(ARGS.w)
            createDirectory(ARGS.w)
        else:
            print >> stderr, "Output directory already exists. Quitting."
            exit(1)
    elif createDirResponse == None and not isdir(ARGS.w):
        print >> stderr, "Check output directory path. Quitting."
        exit(1)

    createDirectory("%s/workspace" % ARGS.w)
    createDirectory("%s/results" % ARGS.w)

    logging.basicConfig(filename='%s/run.log' % ARGS.w,
                        level=LEVEL,
                        format='%(asctime)s %(levelname)s %(message)s',
                        datefmt='%m/%d/%Y %I:%M:%S %p',
                        filemode=LOGMODE)

    WORKSPACE = "%s/workspace" % ARGS.w

    # process PE and SR information
    processFragments()

    # add soft link to the results
    inpt = "%s/variants.pu.bedpe" % WORKSPACE
    otpt = "%s/results/variants.bedpe" % ARGS.w
    symlink(abspath(inpt), abspath(otpt))
    inpt = "%s/variants.pu.vcf" % WORKSPACE
    otpt = "%s/results/variants.vcf" % ARGS.w
    symlink(abspath(inpt), abspath(otpt))

    logging.shutdown()   
